---
title: "Group Project: Spotify" 
author: "Chu Li (1082477), Denis Lebedev (4826973)，Tinghui Xu (1715119), Eleni Spyrou(4515919)"
date: 2025
output:
    prettydoc::html_pretty:
      theme: cayman
      highlight: github
      toc: true
      number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = NA
)

library(readr)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(ISLR)
library(glmnet)
library(tidyverse)
library(caret)
library(corrplot)
```


```{r}
set.seed(123)
Spotify <- read.csv("Spotify-2000.csv")
colnames(Spotify)
```


```{r data-cleaning, warning=FALSE, message=FALSE}
str(Spotify) # check the structure of the dataset
colSums(is.na(Spotify)) # check for missing values in the dataset.
# The dataset has no missing values.

# We then clean the dataset by renaming the columns for clarity and converting the relevant columns to numeric.
# Check which columns are not clean numerics
glimpse(Spotify)

# Cleaned version (only mutate numeric columns that are truly numeric)
spotify_data <- Spotify %>%
  rename(
    BPM = `Beats.Per.Minute..BPM.`,
    Loudness = `Loudness..dB.`,
    Genre = `Top.Genre`,
    Length = `Length..Duration.`
  ) %>%
  mutate(
    BPM = as.numeric(BPM),
    Energy = as.numeric(Energy),
    Danceability = as.numeric(Danceability),
    Loudness = as.numeric(Loudness),
    Valence = as.numeric(Valence),
    Length = as.numeric(Length),
    Acousticness = as.numeric(Acousticness),
    Speechiness = as.numeric(Speechiness),
    Liveness = as.numeric(Liveness),
    Popularity = as.numeric(Popularity),
    Year = as.numeric(Year)
  )
```

## Research question:
Which audio features best explain the difference in popularity of Spotify music from 1956 to 2019, based on the top 2000 tracks?

## Motivation:
In the age of streaming, music popularity is no longer accidental, but can be quantified and analyzed.The Spotify dataset(https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset) provides audio features including energy, danceability, positive emotions, and more.This study examines the 2,000 most popular songs between 1956 and 2019 to analyze which of these features best explain differences in song popularity and to better understand listener preferences in the context of Historical Changes.


# (1) A green histogram showing the distribution of the number of tracks in different popularity ranges.
X-axis: represents the “Popularity Score”, which ranges from 0 to 100 on a scale of 25 (0, 25, 50, 75, 100).
Y-axis: represents the “Count of Tracks”, ranging from 0 to 250 on a scale of 100.

```{r plot-popularity-histogram, fig.align='center', fig.height=5, fig.width = 7}
# Plot histogram of popularity
ggplot(spotify_data, aes(x = Popularity)) +
  geom_histogram(binwidth = 5, fill = "darkgreen", color = "white", alpha = 0.8) +
  labs(
    title = "Distribution of Track Popularity",
    x = "Popularity Score",
    y = "Count of Tracks"
  ) +
  theme_minimal()

```

# Key observations:
  (a) The distribution of the popularity scores is right skewed (positively skewed), i.e., the number of tracks with low popularity (0-25) is low, and the number of tracks with high popularity (75-100) is also low, with the majority of tracks clustered in the 50-75 range. This indicates a slight bimodal character (bimodal) to the distribution, with the main peak in the 60-75 range and the secondary peak in the 50-60 range.
  (b) This suggests that most tracks have a concentration of moderate popularity, with fewer very high or very low popularity, reflecting the trend of concentration in music popularity.



# (2) We decided to plot a line graph to represent the average annual track popularity.
X-axis: represents “Release Year”, which spans from 1960 to 2020, and is scaled in 10-year increments (1960, 1980, 2000, 2020).
Y-axis: represents “Average Popularity”, which spans from 50 to 70 and is scaled in units of 10.
We use a blue line to represent the average popularity of the tracks for each year, and a light blue shaded area around the line to represent the confidence interval, which shows the variability of the data. There is also a blue linear trend line running through the entire chart, reflecting the overall trend.

```{r plot-popularity-year, warning=FALSE, message=FALSE, fig.align='center', fig.height=5, fig.width = 7}
# Plot average popularity by year with linear model
spotify_data %>%
  group_by(Year) %>%
  summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE)) %>%
  ggplot(aes(x = Year, y = Avg_Popularity)) +
  geom_line(color = "steelblue", size = 1) +
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", se = TRUE) + 
  labs(
    title = "Average Track Popularity Over Years",
    x = "Release Year",
    y = "Average Popularity"
  ) +
  theme_minimal()

```
# Key observations:
  (a) From 1960 to 2020, average repertoire prevalence shows a general downward trend, dropping to a low around 2000, near 50. after 2000, the trend begins to stabilize and shows slight fluctuations and signs of increase between 2010 and 2020, with prevalence rising slightly back above 50 in 2020.
  (b) Volatility: Prevalence fluctuates significantly between years. For example, there was a sharp peak in the early 1960s followed by a rapid decline; flatter fluctuations in the 1980s and 1990s; and several small increases and decreases in the late 2000s and early 2010s.
  (c) Confidence intervals: The light blue shaded areas indicate confidence intervals for the mean prevalence, reflecting the variability and statistical reliability of the data. In the 1960s and 2020s, the confidence intervals are narrower, suggesting that the data are less variable in these years and that the data may have larger sample sizes or a more concentrated distribution of prevalence. In contrast, in the 1980s through the 2000s, the confidence intervals are slightly wider, indicating that the data are more variable, possibly due to increased differences in the popularity of different types of music.
  (d) Trend line: The blue linear trend line shows that the long-term trend is downward, with a negative slope. This trend line was fitted by a linear regression model and shows that despite short-term fluctuations, overall average prevalence has declined by about 15-20 points over the past 60 years (from 67 to about 50).


# (3) We decide to draw a heatmap to visualize the correlation between the numeric variables in the dataset. 
  (a) Heatmaps use colors and numbers to indicate the linear correlation between variables, with values ranging from -1 to 1.
  (b) The colors range from red (negative correlation) to blue (positive correlation), and the darker the color, the stronger the correlation.
  (c) The area above the diagonal is blank (only the lower triangle is displayed), which improves simplicity.
  (d) The heatmap will help us identify any strong correlations between features, which can be useful for feature selection in predictive modeling.

```{r plot-correlation-heatmap, fig.align='center', fig.height=5, fig.width = 7}
# Select numeric variables
num_data <- spotify_data %>%
  select(where(is.numeric))

# Compute correlation matrix
cor_matrix <- cor(num_data, use = "complete.obs")

# Plot heatmap
corrplot(cor_matrix, method = "color", type = "lower", 
         tl.col = "black", tl.srt = 45, number.cex = 0.7,
         addCoef.col = "black", diag = FALSE)
```
# Key observations:
  (a) Energy and Loudness have the highest correlation (0.74): music with high energy tends to be louder.
  (b) Acousticness and Energy have the strongest negative correlation (-0.67): music with high acoustics tends to have low energy (softer).
  (c) Popularity has weak correlations with other variables (the absolute value is basically < 0.2), indicating that popularity is affected by more complex factors and is not determined by these audio features alone.


# (4) Linear Model: deviding the dataset into train, valid and test

```{r, fig.align='center', fig.height=5, fig.width = 7}
#get rid of all NA values
spotify_data <- na.omit(spotify_data)

#creating train, test and valid datasets
train_index <- createDataPartition(spotify_data$Popularity, p = 0.5, list = FALSE)
df_train <- spotify_data[train_index, ]

remaining <- spotify_data[-train_index, ]
valid_index <- createDataPartition(remaining$Popularity, p = 0.6, list = FALSE)
df_valid <- remaining[valid_index, ]

df_test <- remaining[-valid_index, ]

#graph to make sure of distribution
df_train$Set <- "Train"
df_valid$Set <- "Validation"
df_test$Set <- "Test"

combined <- bind_rows(df_train, df_valid, df_test)

ggplot(combined, aes(x = Popularity, fill = Set)) +
  geom_density(alpha = 0.4) +
  theme_minimal()
```

# (5) Linear Model: choosing the best model based on MSE

```{r}
#some preparations: functions for formula generation and MSE calculation
source("generate_formulas.R")

lm_mse <- function(formula, train_data, valid_data) {
  y_name <- as.character(formula)[2]
  y_true <- valid_data[[y_name]]
  
  lm_fit <- lm(formula, train_data)
  y_pred <- predict(lm_fit, newdata = valid_data)
  
  mean((y_true - y_pred)^2)
}

# identifying predictors
df_pred <- spotify_data %>% select(-c("Title", "Artist", "Genre", "Popularity", "Index"))
predictors <- colnames(df_pred)

# assessing the best model
top_formulas <- list()
top_mses <- numeric()

for (p in 1:length(predictors)) {
  forms <- generate_formulas(p = p, x_vars = predictors, y_var = "Popularity")
  mses <- rep(0, length(forms))

  for (i in 1:length(forms)) {
    mses[i] <- lm_mse(as.formula(forms[i]), df_train, df_valid)
  }

  top_formulas[[p]] <- forms[which.min(mses)]
  top_mses[p] <- mses[which.min(mses)]
}

#print("Top formulas for each number of predictors")
#top_formulas

print(paste("Best overall formula based on MSE (MSE =", round(min(top_mses), digits = 2), ")"))
best_model <- top_formulas[[which.min(top_mses)]]
best_model
```

# (6) Linear Model: calculating test MSE

```{r}
#calculate test MSE
mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}

train_valid <- bind_rows(df_train, df_valid)
best_model <- lm(best_model, data = train_valid)

pred_test <- predict(best_model, newdata = df_test)
test_mse <- mse(df_test$Popularity, pred_test)

print(paste("Test MSE =", round(test_mse, digits = 2)))
```

# (7) Showing the results graphically

```{r, fig.align='center', fig.height=5, fig.width = 7}
results_df <- data.frame(
  Actual = df_test$Popularity,
  Predicted = pred_test
)

ggplot(results_df, aes(x = Predicted, y = Actual)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_abline(slope = 1, intercept = 0, color = "red", size = 1) +
  labs(title = "Predicted vs Actual Popularity",
       x = "Predicted Popularity",
       y = "Actual Popularity") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

# student’s contribution:

Chu Li(1082477): step 1&2(code+descriptive interpretation), creating GitHub repository
Denis Lebedev (4826973): step 3 (code) + some cosmetic features for all sections
Tinghui Xu (1715119):step 1&2 previous examples and revises 
Eleni Spyrou(4515919): step 3&4 (code) + descriptive interpretation
# AI tools

Using ChatGPT to get inspiration for analyzing ideas
